#!/usr/bin/env python3
"""
Regression and Testing Framework for Quantitative Trading

This module provides comprehensive regression analysis and testing capabilities with:
- Multiple regression methods (OLS, Lasso, Ridge, Elastic Net, Random Forest, XGBoost)
- GPU acceleration support
- Cross-validation and walk-forward analysis
- Multi-dimensional testing across tickers, timeframes, and horizons
- Statistical significance testing
- Performance metrics and visualization

Author: Manus AI
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from scipy import stats
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from joblib import Parallel, delayed
import multiprocessing as mp
from functools import partial
import warnings
warnings.filterwarnings('ignore')

# GPU support
try:
    import cudf
    import cupy as cp
    from cuml.linear_model import LinearRegression as cuLinearRegression
    from cuml.linear_model import Lasso as cuLasso
    from cuml.linear_model import Ridge as cuRidge
    from cuml.ensemble import RandomForestRegressor as cuRandomForestRegressor
    GPU_AVAILABLE = True
except ImportError:
    GPU_AVAILABLE = False

# XGBoost support
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

class RegressionTester:
    """
    Comprehensive regression testing framework for quantitative trading
    """
    
    def __init__(self, use_gpu=False, n_jobs=-1):
        self.use_gpu = use_gpu and GPU_AVAILABLE
        self.n_jobs = n_jobs if n_jobs > 0 else mp.cpu_count()
        self.results_cache = {}
        
        print(f"RegressionTester initialized:")
        print(f"  GPU acceleration: {'✓' if self.use_gpu else '✗'}")
        print(f"  XGBoost available: {'✓' if XGBOOST_AVAILABLE else '✗'}")
        print(f"  Parallel jobs: {self.n_jobs}")
    
    def prepare_regression_data(self, features, returns, min_observations=50, 
                              feature_selection=True, max_features=100):\n        \"\"\"Prepare data for regression analysis\"\"\"\n        # Align features and returns\n        combined_data = pd.concat([features, returns], axis=1).dropna()\n        \n        if len(combined_data) < min_observations:\n            return None, None, None\n        \n        # Separate features and target\n        feature_cols = features.columns\n        return_cols = returns.columns\n        \n        X = combined_data[feature_cols]\n        y = combined_data[return_cols]\n        \n        # Feature selection if requested\n        if feature_selection and len(feature_cols) > max_features:\n            # Remove features with too many NaN values\n            valid_features = X.dropna(axis=1, thresh=len(X) * 0.8)\n            \n            # Remove highly correlated features\n            if len(valid_features.columns) > max_features:\n                corr_matrix = valid_features.corr().abs()\n                upper_tri = corr_matrix.where(\n                    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n                )\n                \n                # Find features with correlation > 0.95\n                high_corr_features = [column for column in upper_tri.columns \n                                    if any(upper_tri[column] > 0.95)]\n                \n                # Remove highly correlated features\n                valid_features = valid_features.drop(columns=high_corr_features[:len(high_corr_features)//2])\n            \n            # Select top features by variance\n            if len(valid_features.columns) > max_features:\n                feature_variance = valid_features.var().sort_values(ascending=False)\n                selected_features = feature_variance.head(max_features).index\n                valid_features = valid_features[selected_features]\n            \n            X = valid_features\n        \n        # Final cleanup\n        X = X.fillna(method='ffill').fillna(0)\n        \n        return X, y, combined_data.index\n    \n    def run_single_regression(self, X, y, method='ols', cv_folds=5, test_size=0.2):\n        \"\"\"Run a single regression analysis\"\"\"\n        if len(X) == 0 or len(y) == 0:\n            return None\n        \n        results = {\n            'method': method,\n            'n_observations': len(X),\n            'n_features': len(X.columns),\n            'feature_names': list(X.columns)\n        }\n        \n        try:\n            # Split data for out-of-sample testing\n            split_idx = int(len(X) * (1 - test_size))\n            X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n            y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n            \n            # Scale features\n            scaler = RobustScaler()\n            X_train_scaled = pd.DataFrame(\n                scaler.fit_transform(X_train),\n                columns=X_train.columns,\n                index=X_train.index\n            )\n            X_test_scaled = pd.DataFrame(\n                scaler.transform(X_test),\n                columns=X_test.columns,\n                index=X_test.index\n            )\n            \n            # Select and train model\n            if self.use_gpu and method in ['ols', 'lasso', 'ridge']:\n                model = self._get_gpu_model(method)\n                # Convert to cuDF for GPU processing\n                X_train_gpu = cudf.from_pandas(X_train_scaled)\n                y_train_gpu = cudf.from_pandas(y_train)\n                model.fit(X_train_gpu, y_train_gpu.iloc[:, 0])  # Single target\n                \n                # Predictions\n                X_test_gpu = cudf.from_pandas(X_test_scaled)\n                y_pred_train = model.predict(X_train_gpu).to_pandas()\n                y_pred_test = model.predict(X_test_gpu).to_pandas()\n                \n                # Get coefficients\n                coefficients = model.coef_.to_pandas() if hasattr(model.coef_, 'to_pandas') else model.coef_\n                \n            else:\n                model = self._get_cpu_model(method)\n                model.fit(X_train_scaled, y_train.iloc[:, 0])  # Single target\n                \n                # Predictions\n                y_pred_train = model.predict(X_train_scaled)\n                y_pred_test = model.predict(X_test_scaled)\n                \n                # Get coefficients (if available)\n                coefficients = getattr(model, 'coef_', None)\n                if coefficients is None:\n                    coefficients = getattr(model, 'feature_importances_', None)\n            \n            # Calculate metrics\n            train_r2 = r2_score(y_train.iloc[:, 0], y_pred_train)\n            test_r2 = r2_score(y_test.iloc[:, 0], y_pred_test)\n            train_mse = mean_squared_error(y_train.iloc[:, 0], y_pred_train)\n            test_mse = mean_squared_error(y_test.iloc[:, 0], y_pred_test)\n            \n            results.update({\n                'train_r2': train_r2,\n                'test_r2': test_r2,\n                'train_mse': train_mse,\n                'test_mse': test_mse,\n                'overfitting': train_r2 - test_r2,\n                'coefficients': dict(zip(X.columns, coefficients)) if coefficients is not None else {},\n                'feature_importance': dict(zip(X.columns, np.abs(coefficients))) if coefficients is not None else {}\n            })\n            \n            # Cross-validation\n            if cv_folds > 1 and len(X) > cv_folds * 10:\n                try:\n                    cv_model = self._get_cpu_model(method)  # Use CPU for CV\n                    tscv = TimeSeriesSplit(n_splits=cv_folds)\n                    cv_scores = cross_val_score(cv_model, X_train_scaled, y_train.iloc[:, 0], \n                                              cv=tscv, scoring='r2')\n                    results['cv_mean'] = cv_scores.mean()\n                    results['cv_std'] = cv_scores.std()\n                except Exception as e:\n                    results['cv_mean'] = np.nan\n                    results['cv_std'] = np.nan\n            \n            # Statistical significance (for linear models)\n            if method in ['ols'] and coefficients is not None:\n                try:\n                    # Calculate t-statistics and p-values\n                    n = len(X_train)\n                    k = len(X_train.columns)\n                    \n                    # Residuals\n                    residuals = y_train.iloc[:, 0] - y_pred_train\n                    mse_resid = np.sum(residuals**2) / (n - k - 1)\n                    \n                    # Standard errors\n                    var_covar_matrix = mse_resid * np.linalg.inv(X_train_scaled.T @ X_train_scaled)\n                    se_coef = np.sqrt(np.diag(var_covar_matrix))\n                    \n                    # t-statistics and p-values\n                    t_stats = coefficients / se_coef\n                    p_values = 2 * (1 - stats.t.cdf(np.abs(t_stats), n - k - 1))\n                    \n                    results['t_statistics'] = dict(zip(X.columns, t_stats))\n                    results['p_values'] = dict(zip(X.columns, p_values))\n                    results['significant_features'] = [col for col, p in zip(X.columns, p_values) if p < 0.05]\n                    \n                except Exception as e:\n                    results['t_statistics'] = {}\n                    results['p_values'] = {}\n                    results['significant_features'] = []\n            \n        except Exception as e:\n            results['error'] = str(e)\n            results.update({\n                'train_r2': 0, 'test_r2': 0, 'train_mse': np.inf, 'test_mse': np.inf,\n                'overfitting': 0, 'coefficients': {}, 'feature_importance': {}\n            })\n        \n        return results\n    \n    def _get_cpu_model(self, method):\n        \"\"\"Get CPU-based model\"\"\"\n        models = {\n            'ols': LinearRegression(),\n            'lasso': Lasso(alpha=0.01, max_iter=1000),\n            'ridge': Ridge(alpha=1.0),\n            'elastic_net': ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=1000),\n            'random_forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),\n            'gradient_boosting': GradientBoostingRegressor(n_estimators=100, max_depth=6, random_state=42)\n        }\n        \n        if method == 'xgboost' and XGBOOST_AVAILABLE:\n            models['xgboost'] = xgb.XGBRegressor(\n                n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42\n            )\n        \n        return models.get(method, LinearRegression())\n    \n    def _get_gpu_model(self, method):\n        \"\"\"Get GPU-based model\"\"\"\n        models = {\n            'ols': cuLinearRegression(),\n            'lasso': cuLasso(alpha=0.01),\n            'ridge': cuRidge(alpha=1.0),\n            'random_forest': cuRandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n        }\n        \n        return models.get(method, cuLinearRegression())\n    \n    def run_cross_sectional_analysis(self, features_dict, returns_dict, \n                                   methods=['lasso', 'ridge', 'random_forest'],\n                                   return_horizons=[1, 5, 10, 20]):\n        \"\"\"Run cross-sectional analysis across multiple tickers\"\"\"\n        print(f\"Running cross-sectional analysis for {len(features_dict)} tickers...\")\n        \n        all_results = []\n        \n        for ticker in features_dict.keys():\n            if ticker not in returns_dict:\n                continue\n            \n            features = features_dict[ticker]\n            returns = returns_dict[ticker]\n            \n            for horizon in return_horizons:\n                return_col = f'forward_return_{horizon}'\n                if return_col not in returns.columns:\n                    continue\n                \n                target_returns = returns[[return_col]]\n                \n                # Prepare data\n                X, y, idx = self.prepare_regression_data(features, target_returns)\n                \n                if X is None:\n                    continue\n                \n                for method in methods:\n                    result = self.run_single_regression(X, y, method=method)\n                    \n                    if result:\n                        result.update({\n                            'ticker': ticker,\n                            'return_horizon': horizon,\n                            'analysis_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n                        })\n                        all_results.append(result)\n        \n        results_df = pd.DataFrame(all_results)\n        print(f\"✓ Completed {len(all_results)} regression analyses\")\n        \n        return results_df\n    \n    def run_time_series_analysis(self, features, returns, methods=['lasso', 'ridge'],\n                               window_size=252, step_size=21):\n        \"\"\"Run time series analysis with rolling windows\"\"\"\n        print(f\"Running time series analysis with {window_size}-day windows...\")\n        \n        all_results = []\n        \n        # Prepare data\n        X, y, idx = self.prepare_regression_data(features, returns)\n        \n        if X is None:\n            print(\"No valid data for time series analysis\")\n            return pd.DataFrame()\n        \n        # Rolling window analysis\n        for start_idx in range(0, len(X) - window_size, step_size):\n            end_idx = start_idx + window_size\n            \n            X_window = X.iloc[start_idx:end_idx]\n            y_window = y.iloc[start_idx:end_idx]\n            \n            window_date = idx[end_idx - 1]\n            \n            for method in methods:\n                result = self.run_single_regression(X_window, y_window, method=method)\n                \n                if result:\n                    result.update({\n                        'window_start': idx[start_idx],\n                        'window_end': window_date,\n                        'window_size': window_size\n                    })\n                    all_results.append(result)\n        \n        results_df = pd.DataFrame(all_results)\n        print(f\"✓ Completed {len(all_results)} time series analyses\")\n        \n        return results_df\n    \n    def run_parallel_analysis(self, analysis_tasks, n_jobs=None):\n        \"\"\"Run multiple regression analyses in parallel\"\"\"\n        if n_jobs is None:\n            n_jobs = self.n_jobs\n        \n        print(f\"Running {len(analysis_tasks)} analyses in parallel using {n_jobs} cores...\")\n        \n        # Execute in parallel\n        results = Parallel(n_jobs=n_jobs, backend='threading')(\n            delayed(self._run_single_analysis_task)(task) \n            for task in analysis_tasks\n        )\n        \n        # Filter out None results\n        valid_results = [r for r in results if r is not None]\n        \n        print(f\"✓ Completed {len(valid_results)} parallel analyses\")\n        \n        return valid_results\n    \n    def _run_single_analysis_task(self, task):\n        \"\"\"Helper function for parallel analysis\"\"\"\n        try:\n            features = task['features']\n            returns = task['returns']\n            method = task.get('method', 'lasso')\n            ticker = task.get('ticker', 'unknown')\n            horizon = task.get('horizon', 1)\n            \n            X, y, idx = self.prepare_regression_data(features, returns)\n            \n            if X is None:\n                return None\n            \n            result = self.run_single_regression(X, y, method=method)\n            \n            if result:\n                result.update({\n                    'ticker': ticker,\n                    'return_horizon': horizon\n                })\n            \n            return result\n            \n        except Exception as e:\n            return {'error': str(e), 'ticker': task.get('ticker', 'unknown')}\n    \n    def analyze_feature_stability(self, results_df, min_appearances=5):\n        \"\"\"Analyze stability of feature importance across different analyses\"\"\"\n        if results_df.empty:\n            return {}\n        \n        feature_stats = {}\n        \n        # Collect all feature importance scores\n        all_features = set()\n        for _, row in results_df.iterrows():\n            if 'feature_importance' in row and isinstance(row['feature_importance'], dict):\n                all_features.update(row['feature_importance'].keys())\n        \n        # Calculate statistics for each feature\n        for feature in all_features:\n            importance_scores = []\n            appearances = 0\n            \n            for _, row in results_df.iterrows():\n                if 'feature_importance' in row and isinstance(row['feature_importance'], dict):\n                    if feature in row['feature_importance']:\n                        importance_scores.append(row['feature_importance'][feature])\n                        appearances += 1\n            \n            if appearances >= min_appearances:\n                feature_stats[feature] = {\n                    'appearances': appearances,\n                    'mean_importance': np.mean(importance_scores),\n                    'std_importance': np.std(importance_scores),\n                    'median_importance': np.median(importance_scores),\n                    'stability_score': np.mean(importance_scores) / (np.std(importance_scores) + 1e-8)\n                }\n        \n        # Sort by stability score\n        stable_features = sorted(\n            feature_stats.items(),\n            key=lambda x: x[1]['stability_score'],\n            reverse=True\n        )\n        \n        return dict(stable_features)\n    \n    def generate_performance_report(self, results_df):\n        \"\"\"Generate comprehensive performance report\"\"\"\n        if results_df.empty:\n            return {}\n        \n        report = {\n            'summary': {},\n            'by_method': {},\n            'by_ticker': {},\n            'by_horizon': {},\n            'top_features': {},\n            'model_comparison': {}\n        }\n        \n        # Overall summary\n        report['summary'] = {\n            'total_analyses': len(results_df),\n            'mean_test_r2': results_df['test_r2'].mean(),\n            'median_test_r2': results_df['test_r2'].median(),\n            'std_test_r2': results_df['test_r2'].std(),\n            'positive_r2_rate': (results_df['test_r2'] > 0).mean(),\n            'significant_r2_rate': (results_df['test_r2'] > 0.01).mean()\n        }\n        \n        # Performance by method\n        if 'method' in results_df.columns:\n            method_stats = results_df.groupby('method').agg({\n                'test_r2': ['mean', 'median', 'std', 'count'],\n                'overfitting': ['mean', 'std']\n            }).round(4)\n            report['by_method'] = method_stats.to_dict()\n        \n        # Performance by ticker\n        if 'ticker' in results_df.columns:\n            ticker_stats = results_df.groupby('ticker').agg({\n                'test_r2': ['mean', 'median', 'count'],\n                'n_observations': 'mean'\n            }).round(4)\n            report['by_ticker'] = ticker_stats.to_dict()\n        \n        # Performance by return horizon\n        if 'return_horizon' in results_df.columns:\n            horizon_stats = results_df.groupby('return_horizon').agg({\n                'test_r2': ['mean', 'median', 'std'],\n                'overfitting': 'mean'\n            }).round(4)\n            report['by_horizon'] = horizon_stats.to_dict()\n        \n        # Top performing features\n        feature_stability = self.analyze_feature_stability(results_df)\n        report['top_features'] = dict(list(feature_stability.items())[:20])\n        \n        # Model comparison\n        if 'method' in results_df.columns:\n            model_comparison = results_df.groupby('method')['test_r2'].agg([\n                'count', 'mean', 'std', 'min', 'max'\n            ]).round(4)\n            model_comparison['rank'] = model_comparison['mean'].rank(ascending=False)\n            report['model_comparison'] = model_comparison.to_dict()\n        \n        return report\n    \n    def detect_regime_changes(self, time_series_results, metric='test_r2', \n                            window=10, threshold=0.02):\n        \"\"\"Detect regime changes in model performance\"\"\"\n        if time_series_results.empty or 'window_end' not in time_series_results.columns:\n            return []\n        \n        # Sort by time\n        sorted_results = time_series_results.sort_values('window_end')\n        \n        # Calculate rolling statistics\n        rolling_mean = sorted_results[metric].rolling(window).mean()\n        rolling_std = sorted_results[metric].rolling(window).std()\n        \n        # Detect significant changes\n        regime_changes = []\n        \n        for i in range(window, len(sorted_results)):\n            current_value = sorted_results[metric].iloc[i]\n            expected_value = rolling_mean.iloc[i-1]\n            expected_std = rolling_std.iloc[i-1]\n            \n            if expected_std > 0:\n                z_score = abs(current_value - expected_value) / expected_std\n                \n                if z_score > 2.0:  # 2 standard deviations\n                    regime_changes.append({\n                        'date': sorted_results['window_end'].iloc[i],\n                        'metric': metric,\n                        'current_value': current_value,\n                        'expected_value': expected_value,\n                        'z_score': z_score,\n                        'change_type': 'improvement' if current_value > expected_value else 'deterioration'\n                    })\n        \n        return regime_changes\n    \n    def save_results(self, results, filename):\n        \"\"\"Save results to file\"\"\"\n        if isinstance(results, pd.DataFrame):\n            results.to_pickle(filename)\n        else:\n            import pickle\n            with open(filename, 'wb') as f:\n                pickle.dump(results, f)\n        \n        print(f\"✓ Results saved to {filename}\")\n    \n    def load_results(self, filename):\n        \"\"\"Load results from file\"\"\"\n        try:\n            # Try loading as DataFrame first\n            results = pd.read_pickle(filename)\n            print(f\"✓ Results loaded from {filename}\")\n            return results\n        except:\n            # Try loading as pickle\n            import pickle\n            with open(filename, 'rb') as f:\n                results = pickle.load(f)\n            print(f\"✓ Results loaded from {filename}\")\n            return results
