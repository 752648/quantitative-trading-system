{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Trading Analysis System\n",
    "\n",
    "A comprehensive system for systematic quantitative trading analysis with multi-core processing and GPU acceleration.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Configuration](#setup)\n",
    "2. [Data Download Module](#data-download)\n",
    "3. [Dataset Visualization](#visualization)\n",
    "4. [Feature Engineering](#feature-engineering)\n",
    "5. [Signal Generation](#signal-generation)\n",
    "6. [Regression Analysis & Testing](#regression-testing)\n",
    "7. [Results Analysis](#results)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration {#setup}\n",
    "\n",
    "Import libraries, configure settings, and initialize the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Data acquisition\n",
    "import yfinance as yf\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine learning and statistics\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "# Parallel processing\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from joblib import Parallel, delayed\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "\n",
    "# GPU acceleration (optional)\n",
    "try:\n",
    "    import cudf\n",
    "    import cupy as cp\n",
    "    from cuml.linear_model import LinearRegression as cuLinearRegression\n",
    "    from cuml.ensemble import RandomForestRegressor as cuRandomForestRegressor\n",
    "    GPU_AVAILABLE = True\n",
    "    print(\"✓ GPU acceleration available (RAPIDS)\")\n",
    "except ImportError:\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"⚠ GPU acceleration not available (install RAPIDS for GPU support)\")\n",
    "\n",
    "# Technical analysis\n",
    "try:\n",
    "    import talib\n",
    "    TALIB_AVAILABLE = True\n",
    "    print(\"✓ TA-Lib available for advanced technical indicators\")\n",
    "except ImportError:\n",
    "    TALIB_AVAILABLE = False\n",
    "    print(\"⚠ TA-Lib not available (install for advanced technical indicators)\")\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"Python version: {os.sys.version}\")\n",
    "print(f\"Available CPU cores: {mp.cpu_count()}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Configuration\n",
    "CONFIG = {\n",
    "    'data_path': './quant_data',\n",
    "    'db_name': 'market_data.db',\n",
    "    'timeframes': ['1d', '1h', '30m', '15m', '5m'],\n",
    "    'return_horizons': [1, 5, 10, 20, 60],\n",
    "    'min_periods': 252,\n",
    "    'n_cores': min(mp.cpu_count(), 8),  # Limit to 8 cores max\n",
    "    'use_gpu': GPU_AVAILABLE,\n",
    "    'chunk_size': 1000,  # For batch processing\n",
    "    'max_memory_gb': 8,  # Memory limit\n",
    "}\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs(CONFIG['data_path'], exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Download Module {#data-download}\n",
    "\n",
    "Download and store market data for multiple tickers across different timeframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    \"\"\"Handles data download, storage, and retrieval with parallel processing\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.db_path = os.path.join(config['data_path'], config['db_name'])\n",
    "        self._init_database()\n",
    "        \n",
    "    def _init_database(self):\n",
    "        \"\"\"Initialize SQLite database with optimized settings\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Enable WAL mode for better concurrent access\n",
    "        cursor.execute('PRAGMA journal_mode=WAL')\n",
    "        cursor.execute('PRAGMA synchronous=NORMAL')\n",
    "        cursor.execute('PRAGMA cache_size=10000')\n",
    "        cursor.execute('PRAGMA temp_store=memory')\n",
    "        \n",
    "        # Create tables\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS market_data (\n",
    "                ticker TEXT,\n",
    "                timeframe TEXT,\n",
    "                datetime TEXT,\n",
    "                open REAL,\n",
    "                high REAL,\n",
    "                low REAL,\n",
    "                close REAL,\n",
    "                volume INTEGER,\n",
    "                adj_close REAL,\n",
    "                PRIMARY KEY (ticker, timeframe, datetime)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS data_quality (\n",
    "                ticker TEXT,\n",
    "                timeframe TEXT,\n",
    "                start_date TEXT,\n",
    "                end_date TEXT,\n",
    "                total_records INTEGER,\n",
    "                missing_records INTEGER,\n",
    "                data_quality_score REAL,\n",
    "                last_updated TEXT,\n",
    "                PRIMARY KEY (ticker, timeframe)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Create indexes for faster queries\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_ticker_timeframe ON market_data(ticker, timeframe)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_datetime ON market_data(datetime)')\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "    def download_single_ticker(self, ticker, period=\"5y\"):\n",
    "        \"\"\"Download data for a single ticker across all timeframes\"\"\"\n",
    "        results = {'ticker': ticker, 'success': False, 'timeframes': {}, 'error': None}\n",
    "        \n",
    "        try:\n",
    "            stock = yf.Ticker(ticker)\n",
    "            \n",
    "            # Download daily data (longest history)\n",
    "            daily_data = stock.history(period=period, interval='1d')\n",
    "            \n",
    "            if daily_data.empty:\n",
    "                results['error'] = \"No daily data available\"\n",
    "                return results\n",
    "            \n",
    "            # Store daily data\n",
    "            self._save_ohlcv_to_db(ticker, '1d', daily_data)\n",
    "            results['timeframes']['1d'] = len(daily_data)\n",
    "            \n",
    "            # Download intraday data (limited periods due to API constraints)\n",
    "            intraday_configs = {\n",
    "                '1h': '730d',   # 2 years\n",
    "                '30m': '60d',   # 2 months\n",
    "                '15m': '60d',   # 2 months\n",
    "                '5m': '60d'     # 2 months\n",
    "            }\n",
    "            \n",
    "            for interval, max_period in intraday_configs.items():\n",
    "                try:\n",
    "                    intraday_data = stock.history(period=max_period, interval=interval)\n",
    "                    if not intraday_data.empty:\n",
    "                        self._save_ohlcv_to_db(ticker, interval, intraday_data)\n",
    "                        results['timeframes'][interval] = len(intraday_data)\n",
    "                except Exception as e:\n",
    "                    results['timeframes'][interval] = f\"Error: {str(e)}\"\n",
    "            \n",
    "            results['success'] = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            results['error'] = str(e)\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def download_multiple_tickers(self, tickers, period=\"5y\", n_jobs=None):\n",
    "        \"\"\"Download data for multiple tickers in parallel\"\"\"\n",
    "        if n_jobs is None:\n",
    "            n_jobs = self.config['n_cores']\n",
    "            \n",
    "        print(f\"Downloading data for {len(tickers)} tickers using {n_jobs} cores...\")\n",
    "        \n",
    "        # Use joblib for parallel processing\n",
    "        results = Parallel(n_jobs=n_jobs, backend='threading')(\n",
    "            delayed(self.download_single_ticker)(ticker, period) \n",
    "            for ticker in tickers\n",
    "        )\n",
    "        \n",
    "        # Process results\n",
    "        successful = [r['ticker'] for r in results if r['success']]\n",
    "        failed = [(r['ticker'], r['error']) for r in results if not r['success']]\n",
    "        \n",
    "        # Update data quality metrics\n",
    "        self._update_data_quality_metrics(results)\n",
    "        \n",
    "        return successful, failed, results\n",
    "    \n",
    "    def _save_ohlcv_to_db(self, ticker, timeframe, data):\n",
    "        \"\"\"Save OHLCV data to database with batch processing\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        \n",
    "        # Prepare data for insertion\n",
    "        data_to_insert = []\n",
    "        for idx, row in data.iterrows():\n",
    "            data_to_insert.append((\n",
    "                ticker,\n",
    "                timeframe,\n",
    "                idx.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                float(row['Open']),\n",
    "                float(row['High']),\n",
    "                float(row['Low']),\n",
    "                float(row['Close']),\n",
    "                int(row['Volume']),\n",
    "                float(row.get('Adj Close', row['Close']))\n",
    "            ))\n",
    "        \n",
    "        # Batch insert\n",
    "        cursor = conn.cursor()\n",
    "        cursor.executemany('''\n",
    "            INSERT OR REPLACE INTO market_data \n",
    "            (ticker, timeframe, datetime, open, high, low, close, volume, adj_close)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', data_to_insert)\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def _update_data_quality_metrics(self, results):\n",
    "        \"\"\"Update data quality metrics in database\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        for result in results:\n",
    "            if result['success']:\n",
    "                ticker = result['ticker']\n",
    "                for timeframe, record_count in result['timeframes'].items():\n",
    "                    if isinstance(record_count, int):\n",
    "                        # Calculate data quality score (simplified)\n",
    "                        quality_score = min(1.0, record_count / 252)  # Based on trading days\n",
    "                        \n",
    "                        cursor.execute('''\n",
    "                            INSERT OR REPLACE INTO data_quality\n",
    "                            (ticker, timeframe, total_records, data_quality_score, last_updated)\n",
    "                            VALUES (?, ?, ?, ?, ?)\n",
    "                        ''', (ticker, timeframe, record_count, quality_score, datetime.now().isoformat()))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def load_data(self, ticker, timeframe, start_date=None, end_date=None):\n",
    "        \"\"\"Load data from database with optional date filtering\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        \n",
    "        query = '''\n",
    "            SELECT datetime, open, high, low, close, volume, adj_close\n",
    "            FROM market_data\n",
    "            WHERE ticker = ? AND timeframe = ?\n",
    "        '''\n",
    "        params = [ticker, timeframe]\n",
    "        \n",
    "        if start_date:\n",
    "            query += ' AND datetime >= ?'\n",
    "            params.append(start_date)\n",
    "        \n",
    "        if end_date:\n",
    "            query += ' AND datetime <= ?'\n",
    "            params.append(end_date)\n",
    "            \n",
    "        query += ' ORDER BY datetime'\n",
    "        \n",
    "        df = pd.read_sql_query(query, conn, params=params)\n",
    "        conn.close()\n",
    "        \n",
    "        if df.empty:\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        df.set_index('datetime', inplace=True)\n",
    "        df.columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj_Close']\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_available_data_summary(self):\n",
    "        \"\"\"Get summary of available data in database\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        \n",
    "        query = '''\n",
    "            SELECT \n",
    "                ticker,\n",
    "                timeframe,\n",
    "                COUNT(*) as records,\n",
    "                MIN(datetime) as start_date,\n",
    "                MAX(datetime) as end_date\n",
    "            FROM market_data\n",
    "            GROUP BY ticker, timeframe\n",
    "            ORDER BY ticker, timeframe\n",
    "        '''\n",
    "        \n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Initialize data manager\n",
    "data_manager = DataManager(CONFIG)\n",
    "print(\"✓ Data manager initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tickers to download\n",
    "# Start with liquid, diverse stocks across sectors\n",
    "\n",
    "TICKER_GROUPS = {\n",
    "    'mega_cap_tech': ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'NVDA', 'TSLA'],\n",
    "    'finance': ['JPM', 'BAC', 'WFC', 'GS', 'MS', 'C', 'USB', 'PNC'],\n",
    "    'healthcare': ['JNJ', 'PFE', 'UNH', 'ABBV', 'MRK', 'TMO', 'ABT', 'LLY'],\n",
    "    'consumer': ['WMT', 'HD', 'PG', 'KO', 'PEP', 'NKE', 'MCD', 'COST'],\n",
    "    'industrial': ['BA', 'CAT', 'GE', 'MMM', 'HON', 'UPS', 'RTX', 'LMT'],\n",
    "    'energy': ['XOM', 'CVX', 'COP', 'EOG', 'SLB', 'MPC', 'VLO', 'PSX'],\n",
    "    'utilities': ['NEE', 'DUK', 'SO', 'D', 'AEP', 'EXC', 'XEL', 'SRE']\n",
    "}\n",
    "\n",
    "# Select tickers for analysis\n",
    "SELECTED_TICKERS = []\n",
    "for group, tickers in TICKER_GROUPS.items():\n",
    "    SELECTED_TICKERS.extend(tickers[:4])  # Take first 4 from each group\n",
    "\n",
    "print(f\"Selected {len(SELECTED_TICKERS)} tickers for analysis:\")\n",
    "for group, tickers in TICKER_GROUPS.items():\n",
    "    group_tickers = [t for t in SELECTED_TICKERS if t in tickers]\n",
    "    print(f\"  {group}: {group_tickers}\")\n",
    "\n",
    "print(f\"\\nTotal tickers: {len(SELECTED_TICKERS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data (this cell can take several minutes)\n",
    "print(\"Starting data download...\")\n",
    "print(f\"This will download {len(SELECTED_TICKERS)} tickers across {len(CONFIG['timeframes'])} timeframes\")\n",
    "print(\"Estimated time: 5-15 minutes depending on network speed\\n\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "successful_tickers, failed_tickers, download_results = data_manager.download_multiple_tickers(\n",
    "    SELECTED_TICKERS, \n",
    "    period=\"5y\",  # 5 years of data\n",
    "    n_jobs=CONFIG['n_cores']\n",
    ")\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"DOWNLOAD SUMMARY\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total time: {duration}\")\n",
    "print(f\"Successful downloads: {len(successful_tickers)}\")\n",
    "print(f\"Failed downloads: {len(failed_tickers)}\")\n",
    "\n",
    "if failed_tickers:\n",
    "    print(\"\\nFailed tickers:\")\n",
    "    for ticker, error in failed_tickers:\n",
    "        print(f\"  {ticker}: {error}\")\n",
    "\n",
    "print(f\"\\nSuccessful tickers: {successful_tickers}\")\n",
    "\n",
    "# Save successful tickers for later use\n",
    "with open(os.path.join(CONFIG['data_path'], 'successful_tickers.json'), 'w') as f:\n",
    "    json.dump(successful_tickers, f)\n",
    "\n",
    "print(f\"\\n✓ Data download completed. {len(successful_tickers)} tickers ready for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Visualization {#visualization}\n",
    "\n",
    "Visualize and analyze the downloaded data quality and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataVisualizer:\n",
    "    \"\"\"Handles data visualization and quality analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, data_manager):\n",
    "        self.data_manager = data_manager\n",
    "        \n",
    "    def plot_data_coverage(self, figsize=(15, 8)):\n",
    "        \"\"\"Plot data coverage across tickers and timeframes\"\"\"\n",
    "        summary = self.data_manager.get_available_data_summary()\n",
    "        \n",
    "        if summary.empty:\n",
    "            print(\"No data available for visualization\")\n",
    "            return\n",
    "        \n",
    "        # Create pivot table for heatmap\n",
    "        pivot_data = summary.pivot(index='ticker', columns='timeframe', values='records')\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        # Heatmap of record counts\n",
    "        sns.heatmap(pivot_data, annot=True, fmt='d', cmap='YlOrRd', ax=ax1)\n",
    "        ax1.set_title('Data Coverage: Number of Records per Ticker/Timeframe')\n",
    "        ax1.set_xlabel('Timeframe')\n",
    "        ax1.set_ylabel('Ticker')\n",
    "        \n",
    "        # Bar plot of total records per ticker\n",
    "        total_records = summary.groupby('ticker')['records'].sum().sort_values(ascending=False)\n",
    "        total_records.plot(kind='bar', ax=ax2)\n",
    "        ax2.set_title('Total Records per Ticker (All Timeframes)')\n",
    "        ax2.set_xlabel('Ticker')\n",
    "        ax2.set_ylabel('Total Records')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def plot_price_overview(self, tickers, timeframe='1d', period_days=252, figsize=(15, 10)):\n",
    "        \"\"\"Plot price overview for selected tickers\"\"\"\n",
    "        n_tickers = len(tickers)\n",
    "        cols = 3\n",
    "        rows = (n_tickers + cols - 1) // cols\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "        if rows == 1:\n",
    "            axes = [axes] if n_tickers == 1 else axes\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        for i, ticker in enumerate(tickers):\n",
    "            if i >= len(axes):\n",
    "                break\n",
    "                \n",
    "            data = self.data_manager.load_data(ticker, timeframe)\n",
    "            \n",
    "            if data.empty:\n",
    "                axes[i].text(0.5, 0.5, f'No data for {ticker}', \n",
    "                           ha='center', va='center', transform=axes[i].transAxes)\n",
    "                axes[i].set_title(f'{ticker} - No Data')\n",
    "                continue\n",
    "            \n",
    "            # Get recent data\n",
    "            recent_data = data.tail(period_days)\n",
    "            \n",
    "            # Plot price and volume\n",
    "            ax1 = axes[i]\n",
    "            ax2 = ax1.twinx()\n",
    "            \n",
    "            # Price\n",
    "            ax1.plot(recent_data.index, recent_data['Close'], color='blue', linewidth=1)\n",
    "            ax1.set_ylabel('Price ($)', color='blue')\n",
    "            ax1.tick_params(axis='y', labelcolor='blue')\n",
    "            \n",
    "            # Volume\n",
    "            ax2.bar(recent_data.index, recent_data['Volume'], alpha=0.3, color='red', width=1)\n",
    "            ax2.set_ylabel('Volume', color='red')\n",
    "            ax2.tick_params(axis='y', labelcolor='red')\n",
    "            \n",
    "            # Calculate stats\n",
    "            returns = recent_data['Close'].pct_change().dropna()\n",
    "            volatility = returns.std() * np.sqrt(252)  # Annualized\n",
    "            total_return = (recent_data['Close'].iloc[-1] / recent_data['Close'].iloc[0] - 1) * 100\n",
    "            \n",
    "            ax1.set_title(f'{ticker} | Return: {total_return:.1f}% | Vol: {volatility:.1f}%')\n",
    "            ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(n_tickers, len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_correlation_matrix(self, tickers, timeframe='1d', period_days=252, figsize=(12, 10)):\n",
    "        \"\"\"Plot correlation matrix of returns\"\"\"\n",
    "        returns_data = {}\n",
    "        \n",
    "        for ticker in tickers:\n",
    "            data = self.data_manager.load_data(ticker, timeframe)\n",
    "            if not data.empty:\n",
    "                recent_data = data.tail(period_days)\n",
    "                returns = recent_data['Close'].pct_change().dropna()\n",
    "                returns_data[ticker] = returns\n",
    "        \n",
    "        if not returns_data:\n",
    "            print(\"No data available for correlation analysis\")\n",
    "            return\n",
    "        \n",
    "        # Create returns DataFrame\n",
    "        returns_df = pd.DataFrame(returns_data)\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = returns_df.corr()\n",
    "        \n",
    "        # Plot heatmap\n",
    "        plt.figure(figsize=figsize)\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "                   cmap='RdBu_r', center=0, vmin=-1, vmax=1)\n",
    "        plt.title(f'Return Correlations ({timeframe} timeframe, last {period_days} periods)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return corr_matrix\n",
    "    \n",
    "    def plot_volume_analysis(self, tickers, timeframe='1d', figsize=(15, 8)):\n",
    "        \"\"\"Analyze volume patterns across tickers\"\"\"\n",
    "        volume_stats = []\n",
    "        \n",
    "        for ticker in tickers:\n",
    "            data = self.data_manager.load_data(ticker, timeframe)\n",
    "            if not data.empty:\n",
    "                volume_stats.append({\n",
    "                    'Ticker': ticker,\n",
    "                    'Avg_Volume': data['Volume'].mean(),\n",
    "                    'Median_Volume': data['Volume'].median(),\n",
    "                    'Volume_Std': data['Volume'].std(),\n",
    "                    'Max_Volume': data['Volume'].max(),\n",
    "                    'Min_Volume': data['Volume'].min()\n",
    "                })\n",
    "        \n",
    "        if not volume_stats:\n",
    "            print(\"No volume data available\")\n",
    "            return\n",
    "        \n",
    "        volume_df = pd.DataFrame(volume_stats)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        # Average volume by ticker\n",
    "        volume_df.set_index('Ticker')['Avg_Volume'].plot(kind='bar', ax=ax1)\n",
    "        ax1.set_title('Average Daily Volume by Ticker')\n",
    "        ax1.set_ylabel('Average Volume')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Volume volatility (coefficient of variation)\n",
    "        volume_df['Volume_CV'] = volume_df['Volume_Std'] / volume_df['Avg_Volume']\n",
    "        volume_df.set_index('Ticker')['Volume_CV'].plot(kind='bar', ax=ax2, color='orange')\n",
    "        ax2.set_title('Volume Volatility (Coefficient of Variation)')\n",
    "        ax2.set_ylabel('Volume CV')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return volume_df\n",
    "    \n",
    "    def create_interactive_dashboard(self, tickers, timeframe='1d'):\n",
    "        \"\"\"Create interactive dashboard using Plotly\"\"\"\n",
    "        # Load data for all tickers\n",
    "        all_data = {}\n",
    "        for ticker in tickers[:10]:  # Limit to 10 for performance\n",
    "            data = self.data_manager.load_data(ticker, timeframe)\n",
    "            if not data.empty:\n",
    "                all_data[ticker] = data.tail(252)  # Last year\n",
    "        \n",
    "        if not all_data:\n",
    "            print(\"No data available for dashboard\")\n",
    "            return\n",
    "        \n",
    "        # Create subplots\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Price Trends', 'Volume Analysis', 'Returns Distribution', 'Volatility'),\n",
    "            specs=[[{\"secondary_y\": True}, {\"secondary_y\": False}],\n",
    "                   [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "        )\n",
    "        \n",
    "        colors = px.colors.qualitative.Set3\n",
    "        \n",
    "        for i, (ticker, data) in enumerate(all_data.items()):\n",
    "            color = colors[i % len(colors)]\n",
    "            \n",
    "            # Normalize prices to 100 for comparison\n",
    "            normalized_price = (data['Close'] / data['Close'].iloc[0]) * 100\n",
    "            \n",
    "            # Price trends\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=data.index, y=normalized_price, name=f'{ticker} Price',\n",
    "                          line=dict(color=color), showlegend=True),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            \n",
    "            # Volume\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=data.index, y=data['Volume'], name=f'{ticker} Volume',\n",
    "                          line=dict(color=color), showlegend=False),\n",
    "                row=1, col=2\n",
    "            )\n",
    "            \n",
    "            # Returns distribution\n",
    "            returns = data['Close'].pct_change().dropna() * 100\n",
    "            fig.add_trace(\n",
    "                go.Histogram(x=returns, name=f'{ticker} Returns', \n",
    "                           opacity=0.7, showlegend=False),\n",
    "                row=2, col=1\n",
    "            )\n",
    "            \n",
    "            # Rolling volatility\n",
    "            rolling_vol = returns.rolling(20).std() * np.sqrt(252)\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=data.index[20:], y=rolling_vol, name=f'{ticker} Volatility',\n",
    "                          line=dict(color=color), showlegend=False),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f'Market Data Dashboard ({timeframe} timeframe)',\n",
    "            height=800,\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        # Update axes labels\n",
    "        fig.update_xaxes(title_text=\"Date\", row=1, col=1)\n",
    "        fig.update_xaxes(title_text=\"Date\", row=1, col=2)\n",
    "        fig.update_xaxes(title_text=\"Daily Returns (%)\", row=2, col=1)\n",
    "        fig.update_xaxes(title_text=\"Date\", row=2, col=2)\n",
    "        \n",
    "        fig.update_yaxes(title_text=\"Normalized Price (Base=100)\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Volume\", row=1, col=2)\n",
    "        fig.update_yaxes(title_text=\"Frequency\", row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"Annualized Volatility (%)\", row=2, col=2)\n",
    "        \n",
    "        fig.show()\n",
    "        \n",
    "        return fig\n",
    "\n",
    "# Initialize visualizer\n",
    "visualizer = DataVisualizer(data_manager)\n",
    "print(\"✓ Data visualizer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load successful tickers\n",
    "try:\n",
    "    with open(os.path.join(CONFIG['data_path'], 'successful_tickers.json'), 'r') as f:\n",
    "        successful_tickers = json.load(f)\n",
    "    print(f\"Loaded {len(successful_tickers)} successful tickers\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No successful tickers file found. Please run the data download section first.\")\n",
    "    successful_tickers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data coverage analysis\n",
    "if successful_tickers:\n",
    "    print(\"Data Coverage Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    data_summary = visualizer.plot_data_coverage()\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\nData Summary Statistics:\")\n",
    "    print(data_summary.groupby('timeframe')['records'].agg(['count', 'mean', 'min', 'max']))\nelse:\n",
    "    print(\"No data available for visualization. Please run data download first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price overview for top tickers\n",
    "if successful_tickers:\n",
    "    print(\"Price Overview Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Show first 12 tickers\n",
    "    sample_tickers = successful_tickers[:12]\n",
    "    visualizer.plot_price_overview(sample_tickers, timeframe='1d', period_days=252)\nelse:\n",
    "    print(\"No data available for price analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "if successful_tickers:\n",
    "    print(\"Correlation Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Analyze correlations for a subset of tickers\n",
    "    sample_tickers = successful_tickers[:15]\n",
    "    corr_matrix = visualizer.plot_correlation_matrix(sample_tickers, timeframe='1d', period_days=252)\n",
    "    \n",
    "    if corr_matrix is not None:\n",
    "        print(\"\\nHighest Correlations:\")\n",
    "        # Get upper triangle of correlation matrix\n",
    "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        high_corr = upper_tri.stack().sort_values(ascending=False).head(10)\n",
    "        for (ticker1, ticker2), corr in high_corr.items():\n",
    "            print(f\"  {ticker1} - {ticker2}: {corr:.3f}\")\nelse:\n",
    "    print(\"No data available for correlation analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume analysis\n",
    "if successful_tickers:\n",
    "    print(\"Volume Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    volume_stats = visualizer.plot_volume_analysis(successful_tickers[:15], timeframe='1d')\n",
    "    \n",
    "    if volume_stats is not None:\n",
    "        print(\"\\nTop 5 by Average Volume:\")\n",
    "        top_volume = volume_stats.nlargest(5, 'Avg_Volume')\n",
    "        for _, row in top_volume.iterrows():\n",
    "            print(f\"  {row['Ticker']}: {row['Avg_Volume']:,.0f}\")\n",
    "        \n",
    "        print(\"\\nMost Volatile Volume (High CV):\")\n",
    "        high_vol_cv = volume_stats.nlargest(5, 'Volume_CV')\n",
    "        for _, row in high_vol_cv.iterrows():\n",
    "            print(f\"  {row['Ticker']}: {row['Volume_CV']:.2f}\")\nelse:\n",
    "    print(\"No data available for volume analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive dashboard (optional - requires plotly)\n",
    "if successful_tickers:\n",
    "    print(\"Creating Interactive Dashboard...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        dashboard = visualizer.create_interactive_dashboard(successful_tickers[:8], timeframe='1d')\n",
    "        print(\"✓ Interactive dashboard created above\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create interactive dashboard: {e}\")\n",
    "        print(\"This is optional and doesn't affect the main analysis\")\nelse:\n",
    "    print(\"No data available for dashboard.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering {#feature-engineering}\n",
    "\n",
    "Generate comprehensive technical features with parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Module will be implemented here\n",
    "print(\"Feature Engineering module - Coming next...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

,
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our custom modules\n",
    "from feature_engineering import FeatureEngineer\n",
    "from signal_generation import SignalGenerator\n",
    "from regression_testing import RegressionTester\n",
    "\n",
    "# Initialize modules\n",
    "feature_engineer = FeatureEngineer(use_gpu=CONFIG['use_gpu'], n_jobs=CONFIG['n_cores'])\n",
    "signal_generator = SignalGenerator(n_jobs=CONFIG['n_cores'])\n",
    "regression_tester = RegressionTester(use_gpu=CONFIG['use_gpu'], n_jobs=CONFIG['n_cores'])\n",
    "\n",
    "print(\"✓ All modules initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering {#feature-engineering}\n",
    "\n",
    "Generate comprehensive technical features with parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load successful tickers if not already loaded\n",
    "if 'successful_tickers' not in locals() or not successful_tickers:\n",
    "    try:\n",
    "        with open(os.path.join(CONFIG['data_path'], 'successful_tickers.json'), 'r') as f:\n",
    "            successful_tickers = json.load(f)\n",
    "        print(f\"Loaded {len(successful_tickers)} successful tickers\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No successful tickers found. Please run data download section first.\")\n",
    "        successful_tickers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Configuration\n",
    "FEATURE_CONFIG = {\n",
    "    'timeframe': '1d',  # Start with daily data\n",
    "    'feature_groups': ['price', 'volume', 'microstructure', 'advanced'],  # Exclude 'talib' if not available\n",
    "    'sample_tickers': successful_tickers[:10],  # Start with 10 tickers for testing\n",
    "    'min_data_points': 252  # Minimum 1 year of data\n",
    "}\n",
    "\n",
    "print(\"Feature Engineering Configuration:\")\n",
    "for key, value in FEATURE_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for feature engineering\n",
    "print(\"Loading market data for feature engineering...\")\n",
    "\n",
    "market_data = {}\n",
    "valid_tickers = []\n",
    "\n",
    "for ticker in FEATURE_CONFIG['sample_tickers']:\n",
    "    data = data_manager.load_data(ticker, FEATURE_CONFIG['timeframe'])\n",
    "    \n",
    "    if not data.empty and len(data) >= FEATURE_CONFIG['min_data_points']:\n",
    "        market_data[ticker] = data\n",
    "        valid_tickers.append(ticker)\n",
    "        print(f\"  ✓ {ticker}: {len(data)} data points\")\n",
    "    else:\n",
    "        print(f\"  ✗ {ticker}: insufficient data ({len(data) if not data.empty else 0} points)\")\n",
    "\n",
    "print(f\"\\nLoaded data for {len(valid_tickers)} tickers: {valid_tickers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute features for all tickers in parallel\n",
    "if market_data:\n",
    "    print(\"Computing features for all tickers...\")\n",
    "    print(f\"This will generate {len(FEATURE_CONFIG['feature_groups'])} feature groups for {len(market_data)} tickers\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Compute features in parallel\n",
    "    features_dict = feature_engineer.compute_features_parallel(\n",
    "        market_data, \n",
    "        feature_groups=FEATURE_CONFIG['feature_groups'],\n",
    "        n_jobs=CONFIG['n_cores']\n",
    "    )\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Processing time: {duration}\")\n",
    "    print(f\"Tickers processed: {len(features_dict)}\")\n",
    "    \n",
    "    if features_dict:\n",
    "        sample_ticker = list(features_dict.keys())[0]\n",
    "        sample_features = features_dict[sample_ticker]\n",
    "        print(f\"Features per ticker: {len(sample_features.columns)}\")\n",
    "        print(f\"Sample ticker ({sample_ticker}): {len(sample_features)} data points\")\n",
    "        \n",
    "        # Get feature statistics\n",
    "        feature_stats = feature_engineer.get_feature_importance_stats(sample_features)\n",
    "        print(f\"\\nFeature categories:\")\n",
    "        for category, count in feature_stats['feature_categories'].items():\n",
    "            if count > 0:\n",
    "                print(f\"  {category}: {count} features\")\n",
    "        \n",
    "        # Save features\n",
    "        features_file = os.path.join(CONFIG['data_path'], 'features_dict.pkl')\n",
    "        with open(features_file, 'wb') as f:\n",
    "            pickle.dump(features_dict, f)\n",
    "        print(f\"\\n✓ Features saved to {features_file}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No market data available for feature engineering\")\n",
    "    features_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature quality and correlations\n",
    "if features_dict:\n",
    "    print(\"Feature Quality Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Analyze first ticker as example\n",
    "    sample_ticker = list(features_dict.keys())[0]\n",
    "    sample_features = features_dict[sample_ticker]\n",
    "    \n",
    "    # Missing data analysis\n",
    "    missing_data = sample_features.isnull().sum().sort_values(ascending=False)\n",
    "    high_missing = missing_data[missing_data > len(sample_features) * 0.1]\n",
    "    \n",
    "    if not high_missing.empty:\n",
    "        print(f\"\\nFeatures with >10% missing data:\")\n",
    "        for feature, missing_count in high_missing.head(10).items():\n",
    "            missing_pct = missing_count / len(sample_features) * 100\n",
    "            print(f\"  {feature}: {missing_pct:.1f}%\")\n",
    "    else:\n",
    "        print(\"\\n✓ No features with excessive missing data\")\n",
    "    \n",
    "    # Feature correlation analysis\n",
    "    numeric_features = sample_features.select_dtypes(include=[np.number])\n",
    "    if len(numeric_features.columns) > 1:\n",
    "        corr_matrix = numeric_features.corr()\n",
    "        \n",
    "        # Find highly correlated feature pairs\n",
    "        high_corr_pairs = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                corr_val = corr_matrix.iloc[i, j]\n",
    "                if abs(corr_val) > 0.9:\n",
    "                    high_corr_pairs.append((\n",
    "                        corr_matrix.columns[i], \n",
    "                        corr_matrix.columns[j], \n",
    "                        corr_val\n",
    "                    ))\n",
    "        \n",
    "        if high_corr_pairs:\n",
    "            print(f\"\\nHighly correlated feature pairs (|r| > 0.9):\")\n",
    "            for feat1, feat2, corr in high_corr_pairs[:10]:\n",
    "                print(f\"  {feat1} - {feat2}: {corr:.3f}\")\n",
    "        else:\n",
    "            print(\"\\n✓ No highly correlated feature pairs found\")\n",
    "    \n",
    "    # Feature variance analysis\n",
    "    feature_variance = numeric_features.var().sort_values(ascending=False)\n",
    "    low_variance = feature_variance[feature_variance < 1e-6]\n",
    "    \n",
    "    if not low_variance.empty:\n",
    "        print(f\"\\nLow variance features (potential constants):\")\n",
    "        for feature in low_variance.head(5).index:\n",
    "            print(f\"  {feature}: variance = {feature_variance[feature]:.2e}\")\n",
    "    else:\n",
    "        print(\"\\n✓ All features have sufficient variance\")\n",
    "\nelse:\n    print(\"No features available for quality analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Signal Generation {#signal-generation}\n",
    "\n",
    "Generate trading signals from technical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal Generation Configuration\n",
    "SIGNAL_CONFIG = {\n",
    "    'include_ml_signals': False,  # Set to True for ML signals (slower)\n",
    "    'return_horizons': [1, 5, 10, 20],  # Forward return periods\n",
    "    'signal_types': ['trend', 'mean_reversion', 'volatility', 'volume', 'pattern']\n",
    "}\n",
    "\n",
    "print(\"Signal Generation Configuration:\")\n",
    "for key, value in SIGNAL_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute forward returns for all tickers\n",
    "if features_dict and market_data:\n",
    "    print(\"Computing forward returns...\")\n",
    "    \n",
    "    returns_dict = {}\n",
    "    \n",
    "    for ticker in features_dict.keys():\n",
    "        if ticker in market_data:\n",
    "            price_data = market_data[ticker]\n",
    "            forward_returns = feature_engineer.compute_forward_returns(\n",
    "                price_data, \n",
    "                horizons=SIGNAL_CONFIG['return_horizons']\n",
    "            )\n",
    "            returns_dict[ticker] = forward_returns\n",
    "            print(f\"  ✓ {ticker}: {len(forward_returns.columns)} return series\")\n",
    "    \n",
    "    print(f\"\\nForward returns computed for {len(returns_dict)} tickers\")\n",
    "    \n",
    "    # Save returns\n",
    "    returns_file = os.path.join(CONFIG['data_path'], 'returns_dict.pkl')\n",
    "    with open(returns_file, 'wb') as f:\n",
    "        pickle.dump(returns_dict, f)\n",
    "    print(f\"✓ Returns saved to {returns_file}\")\n",
    "\nelse:\n    print(\"No features or market data available for return computation\")\n    returns_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate signals for all tickers\n",
    "if features_dict and market_data and returns_dict:\n",
    "    print(\"Generating trading signals...\")\n",
    "    \n",
    "    signals_dict = {}\n",
    "    \n",
    "    for ticker in features_dict.keys():\n",
    "        if ticker in market_data and ticker in returns_dict:\n",
    "            print(f\"\\nProcessing {ticker}...\")\n",
    "            \n",
    "            features = features_dict[ticker]\n",
    "            price_data = market_data[ticker]\n",
    "            returns = returns_dict[ticker]['forward_return_5'] if 'forward_return_5' in returns_dict[ticker].columns else None\n",
    "            \n",
    "            # Generate all signal types\n",
    "            signals = signal_generator.generate_all_signals(\n",
    "                features, \n",
    "                price_data, \n",
    "                returns=returns,\n",
    "                include_ml=SIGNAL_CONFIG['include_ml_signals']\n",
    "            )\n",
    "            \n",
    "            signals_dict[ticker] = signals\n",
    "            print(f\"  ✓ Generated {len(signals.columns)} signals\")\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"SIGNAL GENERATION SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Tickers processed: {len(signals_dict)}\")\n",
    "    \n",
    "    if signals_dict:\n",
    "        sample_ticker = list(signals_dict.keys())[0]\n",
    "        sample_signals = signals_dict[sample_ticker]\n",
    "        print(f\"Signals per ticker: {len(sample_signals.columns)}\")\n",
    "        \n",
    "        # Get signal summary\n",
    "        signal_summary = signal_generator.get_signal_summary(sample_signals)\n",
    "        print(f\"\\nSignal categories:\")\n",
    "        for category, count in signal_summary['signal_types'].items():\n",
    "            print(f\"  {category}: {count} signals\")\n",
    "        \n",
    "        # Save signals\n",
    "        signals_file = os.path.join(CONFIG['data_path'], 'signals_dict.pkl')\n",
    "        with open(signals_file, 'wb') as f:\n",
    "            pickle.dump(signals_dict, f)\n",
    "        print(f\"\\n✓ Signals saved to {signals_file}\")\n",
    "\nelse:\n    print(\"Missing required data for signal generation\")\n    signals_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze signal performance\n",
    "if signals_dict and returns_dict:\n",
    "    print(\"Analyzing signal performance...\")\n",
    "    \n",
    "    # Analyze performance for first ticker as example\n",
    "    sample_ticker = list(signals_dict.keys())[0]\n",
    "    sample_signals = signals_dict[sample_ticker]\n",
    "    sample_returns = returns_dict[sample_ticker]['forward_return_5']\n",
    "    \n",
    "    # Evaluate signal performance\n",
    "    performance_results = signal_generator.evaluate_signal_performance(\n",
    "        sample_signals, \n",
    "        sample_returns, \n",
    "        holding_periods=[5]  # 5-day holding period\n",
    "    )\n",
    "    \n",
    "    # Display top performing signals\n",
    "    print(f\"\\nTop 10 performing signals for {sample_ticker}:\")\n",
    "    \n",
    "    signal_performance_list = []\n",
    "    for signal_name, perf in performance_results.items():\n",
    "        if 'period_5' in perf and perf['period_5']['num_trades'] > 10:\n",
    "            signal_performance_list.append({\n",
    "                'signal': signal_name,\n",
    "                'sharpe_ratio': perf['period_5']['sharpe_ratio'],\n",
    "                'hit_rate': perf['period_5']['hit_rate'],\n",
    "                'num_trades': perf['period_5']['num_trades'],\n",
    "                'total_return': perf['period_5']['total_return']\n",
    "            })\n",
    "    \n",
    "    # Sort by Sharpe ratio\n",
    "    signal_performance_list.sort(key=lambda x: x['sharpe_ratio'], reverse=True)\n",
    "    \n",
    "    for i, signal_perf in enumerate(signal_performance_list[:10], 1):\n",
    "        print(f\"{i:2d}. {signal_perf['signal'][:30]:30s} | \"\n",
    "              f\"Sharpe: {signal_perf['sharpe_ratio']:6.3f} | \"\n",
    "              f\"Hit Rate: {signal_perf['hit_rate']:5.1%} | \"\n",
    "              f\"Trades: {signal_perf['num_trades']:3d}\")\n",
    "\nelse:\n    print(\"No signals or returns available for performance analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regression Analysis & Testing {#regression-testing}\n",
    "\n",
    "Run comprehensive regression analysis to identify predictive patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Testing Configuration\n",
    "REGRESSION_CONFIG = {\n",
    "    'methods': ['lasso', 'ridge', 'random_forest'],  # Regression methods to test\n",
    "    'return_horizons': [1, 5, 10],  # Forward return periods to predict\n",
    "    'cross_validation': True,  # Enable cross-validation\n",
    "    'feature_selection': True,  # Enable automatic feature selection\n",
    "    'max_features': 50,  # Maximum features per regression\n",
    "    'min_observations': 100  # Minimum observations required\n",
    "}\n",
    "\n",
    "print(\"Regression Testing Configuration:\")\n",
    "for key, value in REGRESSION_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross-sectional regression analysis\n",
    "if features_dict and returns_dict:\n",
    "    print(\"Running cross-sectional regression analysis...\")\n",
    "    print(f\"This will test {len(REGRESSION_CONFIG['methods'])} methods across {len(REGRESSION_CONFIG['return_horizons'])} horizons\")\n",
    "    print(f\"for {len(features_dict)} tickers\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Run cross-sectional analysis\n",
    "    regression_results = regression_tester.run_cross_sectional_analysis(\n",
    "        features_dict,\n",
    "        returns_dict,\n",
    "        methods=REGRESSION_CONFIG['methods'],\n",
    "        return_horizons=REGRESSION_CONFIG['return_horizons']\n",
    "    )\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"REGRESSION ANALYSIS SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Processing time: {duration}\")\n",
    "    print(f\"Total regressions: {len(regression_results)}\")\n",
    "    \n",
    "    if not regression_results.empty:\n",
    "        # Display summary statistics\n",
    "        print(f\"\\nPerformance Summary:\")\n",
    "        print(f\"  Mean test R²: {regression_results['test_r2'].mean():.4f}\")\n",
    "        print(f\"  Median test R²: {regression_results['test_r2'].median():.4f}\")\n",
    "        print(f\"  Positive R² rate: {(regression_results['test_r2'] > 0).mean():.1%}\")\n",
    "        print(f\"  Significant R² rate (>1%): {(regression_results['test_r2'] > 0.01).mean():.1%}\")\n",
    "        \n",
    "        # Performance by method\n",
    "        print(f\"\\nPerformance by Method:\")\n",
    "        method_performance = regression_results.groupby('method')['test_r2'].agg(['count', 'mean', 'std']).round(4)\n",
    "        print(method_performance)\n",
    "        \n",
    "        # Performance by horizon\n",
    "        print(f\"\\nPerformance by Return Horizon:\")\n",
    "        horizon_performance = regression_results.groupby('return_horizon')['test_r2'].agg(['count', 'mean', 'std']).round(4)\n",
    "        print(horizon_performance)\n",
    "        \n",
    "        # Save results\n",
    "        results_file = os.path.join(CONFIG['data_path'], 'regression_results.pkl')\n",
    "        regression_tester.save_results(regression_results, results_file)\n",
    "        \n",
    "    else:\n",
    "        print(\"No regression results obtained\")\n",
    "\nelse:\n    print(\"Missing required data for regression analysis\")\n    regression_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature stability and importance\n",
    "if not regression_results.empty:\n",
    "    print(\"Analyzing feature stability...\")\n",
    "    \n",
    "    # Analyze feature stability across different regressions\n",
    "    feature_stability = regression_tester.analyze_feature_stability(\n",
    "        regression_results, \n",
    "        min_appearances=3\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTop 15 most stable features:\")\n",
    "    print(f\"{'Feature':<30} {'Appearances':<12} {'Mean Imp.':<10} {'Stability':<10}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for i, (feature, stats) in enumerate(list(feature_stability.items())[:15], 1):\n",
    "        print(f\"{feature[:29]:<30} {stats['appearances']:<12} \"\n",
    "              f\"{stats['mean_importance']:<10.4f} {stats['stability_score']:<10.2f}\")\n",
    "    \n",
    "    # Generate comprehensive performance report\n",
    "    performance_report = regression_tester.generate_performance_report(regression_results)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"COMPREHENSIVE PERFORMANCE REPORT\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    summary = performance_report['summary']\n",
    "    print(f\"Total analyses: {summary['total_analyses']}\")\n",
    "    print(f\"Mean test R²: {summary['mean_test_r2']:.4f}\")\n",
    "    print(f\"Positive R² rate: {summary['positive_r2_rate']:.1%}\")\n",
    "    print(f\"Significant R² rate: {summary['significant_r2_rate']:.1%}\")\n",
    "    \n",
    "    # Save comprehensive report\n",
    "    report_file = os.path.join(CONFIG['data_path'], 'performance_report.pkl')\n",
    "    with open(report_file, 'wb') as f:\n",
    "        pickle.dump(performance_report, f)\n",
    "    print(f\"\\n✓ Performance report saved to {report_file}\")\n",
    "\nelse:\n    print(\"No regression results available for feature analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best performing combinations\n",
    "if not regression_results.empty:\n",
    "    print(\"Identifying best performing combinations...\")\n",
    "    \n",
    "    # Filter for meaningful results\n",
    "    significant_results = regression_results[regression_results['test_r2'] > 0.01]\n",
    "    \n",
    "    if not significant_results.empty:\n",
    "        print(f\"\\nTop 10 performing ticker-method-horizon combinations:\")\n",
    "        print(f\"{'Ticker':<8} {'Method':<15} {'Horizon':<8} {'Test R²':<10} {'Features':<10} {'Obs':<8}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        top_results = significant_results.nlargest(10, 'test_r2')\n",
    "        \n",
    "        for _, row in top_results.iterrows():\n",
    "            print(f\"{row['ticker']:<8} {row['method']:<15} {row['return_horizon']:<8} \"\n",
    "                  f\"{row['test_r2']:<10.4f} {row['n_features']:<10} {row['n_observations']:<8}\")\n",
    "        \n",
    "        # Analyze what makes these combinations successful\n",
    "        print(f\"\\nCharacteristics of top performers:\")\n",
    "        print(f\"  Average test R²: {top_results['test_r2'].mean():.4f}\")\n",
    "        print(f\"  Average features used: {top_results['n_features'].mean():.1f}\")\n",
    "        print(f\"  Average observations: {top_results['n_observations'].mean():.0f}\")\n",
    "        print(f\"  Most common method: {top_results['method'].mode().iloc[0]}\")\n",
    "        print(f\"  Most common horizon: {top_results['return_horizon'].mode().iloc[0]}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No significant results found (R² > 1%)\")\n",
    "        \n",
    "        # Show best available results\n",
    "        if len(regression_results) > 0:\n",
    "            print(f\"\\nBest available results:\")\n",
    "            best_results = regression_results.nlargest(5, 'test_r2')\n",
    "            for _, row in best_results.iterrows():\n",
    "                print(f\"  {row['ticker']} - {row['method']} - {row['return_horizon']}d: R² = {row['test_r2']:.4f}\")\n",
    "\nelse:\n    print(\"No regression results available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Analysis {#results}\n",
    "\n",
    "Comprehensive analysis and visualization of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results visualization\n",
    "if not regression_results.empty:\n",
    "    print(\"Creating results visualizations...\")\n",
    "    \n",
    "    # Set up plotting\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Quantitative Trading Analysis Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. R² distribution\n",
    "    axes[0, 0].hist(regression_results['test_r2'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].axvline(regression_results['test_r2'].mean(), color='red', linestyle='--', \n",
    "                      label=f'Mean: {regression_results[\"test_r2\"].mean():.4f}')\n",
    "    axes[0, 0].set_xlabel('Test R²')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Distribution of Test R² Scores')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Performance by method\n",
    "    method_means = regression_results.groupby('method')['test_r2'].mean().sort_values(ascending=True)\n",
    "    method_means.plot(kind='barh', ax=axes[0, 1], color='lightcoral')\n",
    "    axes[0, 1].set_xlabel('Mean Test R²')\n",
    "    axes[0, 1].set_title('Performance by Method')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Performance by return horizon\n",
    "    horizon_means = regression_results.groupby('return_horizon')['test_r2'].mean()\n",
    "    horizon_means.plot(kind='bar', ax=axes[1, 0], color='lightgreen')\n",
    "    axes[1, 0].set_xlabel('Return Horizon (days)')\n",
    "    axes[1, 0].set_ylabel('Mean Test R²')\n",
    "    axes[1, 0].set_title('Performance by Return Horizon')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=0)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Overfitting analysis\n",
    "    axes[1, 1].scatter(regression_results['train_r2'], regression_results['test_r2'], \n",
    "                      alpha=0.6, color='purple')\n",
    "    # Add diagonal line (perfect generalization)\n",
    "    max_r2 = max(regression_results['train_r2'].max(), regression_results['test_r2'].max())\n",
    "    axes[1, 1].plot([0, max_r2], [0, max_r2], 'r--', alpha=0.8, label='Perfect Generalization')\n",
    "    axes[1, 1].set_xlabel('Train R²')\n",
    "    axes[1, 1].set_ylabel('Test R²')\n",
    "    axes[1, 1].set_title('Overfitting Analysis')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_file = os.path.join(CONFIG['data_path'], 'results_analysis.png')\n",
    "    plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ Results visualization saved to {plot_file}\")\n",
    "\nelse:\n    print(\"No regression results available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUANTITATIVE TRADING ANALYSIS - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Data summary\n",
    "print(f\"\\n📊 DATA SUMMARY:\")\n",
    "if 'successful_tickers' in locals():\n",
    "    print(f\"  • Total tickers analyzed: {len(successful_tickers)}\")\nif 'features_dict' in locals():\n    print(f\"  • Tickers with features: {len(features_dict)}\")\nif 'signals_dict' in locals():\n    print(f\"  • Tickers with signals: {len(signals_dict)}\")\n\n# Feature engineering summary\nprint(f\"\\n🔧 FEATURE ENGINEERING:\")\nif features_dict:\n    sample_features = list(features_dict.values())[0]\n    print(f\"  • Features per ticker: {len(sample_features.columns)}\")\n    print(f\"  • Feature categories: {len(FEATURE_CONFIG['feature_groups'])}\")\n    print(f\"  • Processing time: Multi-core parallel processing\")\nelse:\n    print(f\"  • No features generated\")\n\n# Signal generation summary\nprint(f\"\\n📡 SIGNAL GENERATION:\")\nif signals_dict:\n    sample_signals = list(signals_dict.values())[0]\n    print(f\"  • Signals per ticker: {len(sample_signals.columns)}\")\n    print(f\"  • Signal types: {len(SIGNAL_CONFIG['signal_types'])}\")\n    print(f\"  • Return horizons: {SIGNAL_CONFIG['return_horizons']}\")\nelse:\n    print(f\"  • No signals generated\")\n\n# Regression analysis summary\nprint(f\"\\n📈 REGRESSION ANALYSIS:\")\nif not regression_results.empty:\n    print(f\"  • Total regressions: {len(regression_results)}\")\n    print(f\"  • Methods tested: {REGRESSION_CONFIG['methods']}\")\n    print(f\"  • Mean test R²: {regression_results['test_r2'].mean():.4f}\")\n    print(f\"  • Positive R² rate: {(regression_results['test_r2'] > 0).mean():.1%}\")\n    print(f\"  • Significant results (R² > 1%): {(regression_results['test_r2'] > 0.01).mean():.1%}\")\n    \n    # Best performing combination\n    best_result = regression_results.loc[regression_results['test_r2'].idxmax()]\n    print(f\"  • Best result: {best_result['ticker']} - {best_result['method']} - {best_result['return_horizon']}d (R² = {best_result['test_r2']:.4f})\")\nelse:\n    print(f\"  • No regression results\")\n\n# Key findings\nprint(f\"\\n🎯 KEY FINDINGS:\")\nif not regression_results.empty:\n    # Most effective method\n    best_method = regression_results.groupby('method')['test_r2'].mean().idxmax()\n    print(f\"  • Most effective method: {best_method}\")\n    \n    # Most predictable horizon\n    best_horizon = regression_results.groupby('return_horizon')['test_r2'].mean().idxmax()\n    print(f\"  • Most predictable horizon: {best_horizon} days\")\n    \n    # Feature stability\n    if 'feature_stability' in locals() and feature_stability:\n        most_stable_feature = list(feature_stability.keys())[0]\n        print(f\"  • Most stable feature: {most_stable_feature}\")\n    \n    # Overfitting assessment\n    avg_overfitting = regression_results['overfitting'].mean()\n    if avg_overfitting > 0.05:\n        print(f\"  • ⚠️  High overfitting detected (avg: {avg_overfitting:.4f})\")\n    else:\n        print(f\"  • ✅ Low overfitting (avg: {avg_overfitting:.4f})\")\nelse:\n    print(f\"  • Analysis incomplete - no regression results\")\n\n# Recommendations\nprint(f\"\\n💡 RECOMMENDATIONS:\")\nif regression_results.empty:\n    print(f\"  • Increase data sample size\")\n    print(f\"  • Check feature quality and completeness\")\n    print(f\"  • Verify data alignment between features and returns\")\nelse:\n    significant_rate = (regression_results['test_r2'] > 0.01).mean()\n    if significant_rate < 0.1:\n        print(f\"  • Low predictability detected - consider:\")\n        print(f\"    - Adding more sophisticated features\")\n        print(f\"    - Trying different timeframes\")\n        print(f\"    - Implementing regime-aware models\")\n    else:\n        print(f\"  • Promising results detected - next steps:\")\n        print(f\"    - Implement walk-forward validation\")\n        print(f\"    - Add transaction cost modeling\")\n        print(f\"    - Build portfolio construction framework\")\n        print(f\"    - Test on out-of-sample data\")\n\n# Files generated\nprint(f\"\\n📁 FILES GENERATED:\")\ndata_files = [\n    'successful_tickers.json',\n    'features_dict.pkl',\n    'returns_dict.pkl', \n    'signals_dict.pkl',\n    'regression_results.pkl',\n    'performance_report.pkl',\n    'results_analysis.png'\n]\n\nfor file in data_files:\n    file_path = os.path.join(CONFIG['data_path'], file)\n    if os.path.exists(file_path):\n        file_size = os.path.getsize(file_path) / 1024 / 1024  # MB\n        print(f\"  • {file} ({file_size:.1f} MB)\")\n\nprint(f\"\\n✅ Analysis completed successfully!\")\nprint(f\"📂 All results saved to: {CONFIG['data_path']}\")\nprint(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
